{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup (Installation and importing modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will install all the libraries required for the preprocessing of the documents. We will be installing following packages:\n",
    "\n",
    "- contractions==0.0.25\n",
    "- ekphrasis==0.5.1\n",
    "- tensorflow==2.1.0\n",
    "- pandas==0.24.2\n",
    "- numpy==1.18.1\n",
    "- scikit-learn==0.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree # for processing xml\n",
    "import re\n",
    "import glob # file handling\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import contractions # for expanding contractions\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer # For tokenizing documents\n",
    "from ekphrasis.dicts.emoticons import emoticons # for processing emoticons\n",
    "\n",
    "from nltk.corpus import wordnet # for nltk corpus\n",
    "from nltk.stem import WordNetLemmatizer # for Lemmatizaton\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will load the label file and extract document ids present in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df = pd.read_csv(os.path.join('all_data', 'train_labels.csv'))\n",
    "train_files = train_labels_df['id'].to_list() # get training data id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets extract labels for test data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_df = pd.read_csv(os.path.join('all_data', 'test_labels.csv'))\n",
    "test_files = test_labels_df['id'].to_list() # get test data id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parse xml into dictionary where each key represents the id of xml and the values are content of documents for a particular id in the xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = train_files+test_files # consists of all ids of entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join('all_data','data')\n",
    "document_dict = dict()\n",
    "document_test_dict = dict()\n",
    "\n",
    "# store content of each xml into dictionary\n",
    "for file_name in all_files:\n",
    "    doc_list = []\n",
    "    file_path = os.path.join(base_path, file_name+'.xml')\n",
    "    tree = etree.parse(file_path) # parse xml tree\n",
    "    root = tree.getroot() # get root element of tree\n",
    "    # store each content of each doument element in a list\n",
    "    for doc in root.iter('document'): \n",
    "        doc_list.append(doc.text)\n",
    "    if file_name in test_files:\n",
    "        document_test_dict[file_name] = doc_list # store in test data dictionary\n",
    "    else:\n",
    "        document_dict[file_name] = doc_list # store in train data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all special characters in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ee40b86368137b86f51806c9f105b34b</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>919bc742d9a22d65eab1f52b11656cab</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15b97a08d65f22d97ca685686510b6ae</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>affa98421ef5c46ca7c8f246e0a134c1</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  gender\n",
       "0  d7d392835f50664fc079f0f388e147a0    male\n",
       "1  ee40b86368137b86f51806c9f105b34b  female\n",
       "2  919bc742d9a22d65eab1f52b11656cab    male\n",
       "3  15b97a08d65f22d97ca685686510b6ae  female\n",
       "4  affa98421ef5c46ca7c8f246e0a134c1  female"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below regular expression will extract all special characters from all documents and store it.\n",
    "\n",
    "```python\n",
    "re.findall('([^\\u0000-\\u007F]+)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialChars = []\n",
    "\n",
    "# Iterate over each document file to store special characters\n",
    "for id, doc_list in document_dict.items():\n",
    "    for doc in doc_list:\n",
    "        specialChars.extend(re.findall('([^\\u0000-\\u007F]+)', doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common special characters/ emojis are displayed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('…', 22137),\n",
       " ('’', 7174),\n",
       " ('😂', 2411),\n",
       " ('“', 1823),\n",
       " ('”', 1746),\n",
       " ('‘', 956),\n",
       " ('😊', 952),\n",
       " ('–', 886),\n",
       " ('😂😂', 872),\n",
       " ('—', 862),\n",
       " ('\\xa0', 819),\n",
       " ('😍', 738),\n",
       " ('❤️', 658),\n",
       " ('😂😂😂', 652),\n",
       " ('😉', 642),\n",
       " ('£', 607),\n",
       " ('👍', 525),\n",
       " ('😭', 491),\n",
       " ('❤', 480),\n",
       " ('🤔', 471),\n",
       " ('🙄', 467),\n",
       " ('😘', 396),\n",
       " ('é', 389),\n",
       " ('•', 386),\n",
       " ('😀', 380),\n",
       " ('😁', 332),\n",
       " ('😳', 329),\n",
       " ('🙈', 320),\n",
       " ('😢', 305),\n",
       " ('€', 291),\n",
       " ('😜', 289),\n",
       " ('😩', 284),\n",
       " ('💕', 262),\n",
       " ('😎', 260),\n",
       " ('📷', 235),\n",
       " ('😄', 233),\n",
       " ('⚡️', 231),\n",
       " ('😬', 225),\n",
       " ('☺️', 222),\n",
       " ('👍🏻', 212),\n",
       " ('🙃', 211),\n",
       " ('👌', 210),\n",
       " ('😂😂😂😂', 208),\n",
       " ('😃', 199),\n",
       " ('😅', 194),\n",
       " ('♫', 193),\n",
       " ('👀', 185),\n",
       " ('😕', 182),\n",
       " ('🔥', 172),\n",
       " ('😏', 171),\n",
       " ('😡', 169),\n",
       " ('😱', 166),\n",
       " ('😒', 165),\n",
       " ('😔', 162),\n",
       " ('💜', 159),\n",
       " ('🎉', 157),\n",
       " ('☺', 154),\n",
       " ('✨', 144),\n",
       " ('😋', 140),\n",
       " ('💖', 139),\n",
       " ('👌🏻', 137),\n",
       " ('🎄', 136),\n",
       " ('👏', 136),\n",
       " ('😭😭', 135),\n",
       " ('🙂', 132),\n",
       " ('🎶', 131),\n",
       " ('😆', 131),\n",
       " ('😴', 130),\n",
       " ('👌🏼', 127),\n",
       " ('🙌🏻', 127),\n",
       " ('💔', 126),\n",
       " ('🇺🇸', 126),\n",
       " ('😫', 123),\n",
       " ('😝', 122),\n",
       " ('✌', 121),\n",
       " ('💙', 120),\n",
       " ('……', 120),\n",
       " ('😍😍', 117),\n",
       " ('😐', 115),\n",
       " ('🙌', 112),\n",
       " ('😞', 111),\n",
       " ('🤗', 108),\n",
       " ('😍😍😍', 107),\n",
       " ('👍🏼', 106),\n",
       " ('😭😭😭', 104),\n",
       " ('á', 103),\n",
       " ('í', 101),\n",
       " ('❤️❤️', 98),\n",
       " ('💚', 98),\n",
       " ('😑', 93),\n",
       " ('🙏', 91),\n",
       " ('ā', 91),\n",
       " ('😇', 88),\n",
       " ('🌟🌟🌟🌟🌟', 84),\n",
       " ('😣', 80),\n",
       " ('🍻', 80),\n",
       " ('😌', 77),\n",
       " ('😷', 77),\n",
       " ('😂😂😂😂😂', 75),\n",
       " ('💗', 74),\n",
       " ('👊', 74),\n",
       " ('🙌🏼', 73),\n",
       " ('☹️', 71),\n",
       " ('💋', 69),\n",
       " ('🇨🇦', 68),\n",
       " ('😥', 66),\n",
       " ('¯', 66),\n",
       " ('❤❤', 64),\n",
       " ('💯', 63),\n",
       " ('°', 62),\n",
       " ('✅', 62),\n",
       " ('👏🏻', 61),\n",
       " ('💪', 60),\n",
       " ('🙊', 59),\n",
       " ('🤣', 59),\n",
       " ('😛', 58),\n",
       " ('🙏🏻', 58),\n",
       " ('🤓', 58),\n",
       " ('❤️❤️❤️', 57),\n",
       " ('😻', 56),\n",
       " ('❤❤❤', 55),\n",
       " ('😪', 54),\n",
       " ('👏👏👏', 54),\n",
       " ('👏🏼', 53),\n",
       " ('🙁', 52),\n",
       " ('😈', 52),\n",
       " ('💩', 52),\n",
       " ('ó', 52),\n",
       " ('😤', 51),\n",
       " ('🇦🇺', 51),\n",
       " ('😠', 50),\n",
       " ('™', 49),\n",
       " ('❄️', 48),\n",
       " ('☀️', 48),\n",
       " ('⭐️', 47),\n",
       " ('É', 47),\n",
       " ('🙌🏽', 47),\n",
       " ('💛', 47),\n",
       " ('🐶', 47),\n",
       " ('・・・', 47),\n",
       " ('💁🏼', 46),\n",
       " ('👑', 45),\n",
       " ('ú', 45),\n",
       " ('̶', 45),\n",
       " ('🤔🤔', 44),\n",
       " ('⚽️', 43),\n",
       " ('💰', 43),\n",
       " ('✔️', 43),\n",
       " ('👏🏽', 43),\n",
       " ('😰', 43),\n",
       " ('🙄🙄', 43),\n",
       " ('☕️', 43),\n",
       " ('😖', 43),\n",
       " ('😵', 42),\n",
       " ('😓', 42),\n",
       " ('😩😩', 42),\n",
       " ('👎', 42),\n",
       " ('⚡', 42),\n",
       " ('⊕', 42),\n",
       " ('🔴', 41),\n",
       " ('😨', 40),\n",
       " ('🍀', 40),\n",
       " ('💪🏻', 40),\n",
       " ('ﷺ', 40),\n",
       " ('💪🏼', 39),\n",
       " ('😊😊', 39),\n",
       " ('🍷', 38),\n",
       " ('è', 38),\n",
       " ('👍👍', 38),\n",
       " ('👇', 38),\n",
       " ('🙈🙈', 38),\n",
       " ('👌🏽', 37),\n",
       " ('😭😭😭😭', 37),\n",
       " ('👏👏', 37),\n",
       " ('😶', 37),\n",
       " ('🇮🇪', 37),\n",
       " ('⬆', 37),\n",
       " ('🔥🔥🔥', 36),\n",
       " ('🌸', 36),\n",
       " ('😇😇😇', 36),\n",
       " ('😮', 34),\n",
       " ('🏆', 34),\n",
       " ('😘😘', 34),\n",
       " ('ツ', 33),\n",
       " ('🙃🙃🙃', 33),\n",
       " ('🙃🙃', 33),\n",
       " ('😂🙈', 33),\n",
       " ('🍾', 32),\n",
       " ('😯', 32),\n",
       " ('🌹', 32),\n",
       " ('😟', 32),\n",
       " ('💞', 32),\n",
       " ('🙈🙈🙈', 32),\n",
       " ('📸', 31),\n",
       " ('🔥🔥', 31),\n",
       " ('💥', 31),\n",
       " ('💕💕', 31),\n",
       " ('✈️', 31),\n",
       " ('🌎', 30),\n",
       " ('👏🏻👏🏻', 30)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(specialChars).most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems lots of people have used emojis in their tweets, which was expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will collect all external data necessary for preprocessing the documents\n",
    "\n",
    "We will first collect the dictioanry for unicode emojis replacement. The dictionary looks as follows:\n",
    "\n",
    "```python\n",
    "UNICODE_EMO = {'😜': ':face_with_stuck-out_tongue_&_winking_eye:',\n",
    "               '😂': ':face_with_tears_of_joy:',\n",
    "               '🤒': ':face_with_thermometer:',\n",
    "               '😶': ':face_without_mouth:',\n",
    "               '🏭': ':factory:',\n",
    "               '🍂': ':fallen_leaf:',\n",
    "               '👪': ':family:',\n",
    "              .....}\n",
    "\n",
    "```\n",
    "\n",
    "This was downloaded from [here](https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py)\n",
    "\n",
    "Also, some part of the dictionary was modified manually, by removing color of the skin present in the emoji (Just to be anti-racial) :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect unicode repacement dictionary\n",
    "with open('unicode_emo.pickle', 'rb') as handle:\n",
    "    UNICODE_EMO = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A small demo on how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game is on :fire: <happy> :face with tears of joy: and :face with tears of joy::face with tears of joy::face with tears of joy:\n"
     ]
    }
   ],
   "source": [
    "text = \"game is on 🔥 :) 😂 and 😂😂😂\"\n",
    "\n",
    "# replacement from the above dictioanry for emojis\n",
    "for emo, equivalent in UNICODE_EMO.items():\n",
    "    text = text.replace(emo,equivalent).replace('_',' ')\n",
    "    \n",
    "# replacement for emoticons with tags\n",
    "for emot, emot_words in emoticons.items():\n",
    "    text = text.replace(emot,emot_words)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After carefully studying the most common special characters, I decided to form the below dictionary for the replacement of several characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_dict = {'‘':'\\'', \n",
    "                '’':'\\'',\n",
    "                '“':'\\\"',\n",
    "                '”':'\\\"',\n",
    "                '–': '-',\n",
    "                '—':'-',\n",
    "                'é':'e',\n",
    "                'è':'e',\n",
    "                'ú':'us',\n",
    "                '🇺🇸':'US',\n",
    "                'á':'a',\n",
    "                'í':'i',\n",
    "                'ā':'a',\n",
    "                '🇨🇦':'CA',\n",
    "                '°':'degree ',\n",
    "                '🇦🇺':'AU',\n",
    "                'ó':'o',\n",
    "                'É':'E',\n",
    "                'ñ':'n',\n",
    "                '1/3': '',\n",
    "                '2/3': '',\n",
    "                '1/2': '',\n",
    "                '2/2': '',\n",
    "                '3/3': '',\n",
    "                '...': '.',\n",
    "                '..': '.',\n",
    "                '&': 'and '}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small experiment was conducted after initial preprocessing to look into which words are most occuring, for which emebddings were not found. This experiment can be viewed in **experiment.ipynb notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these words were found by words coverage experiment \n",
    "replacement_dict = {'gr8': 'great',\n",
    "                    'lmao': 'laughing',\n",
    "                    'auspol': 'Australian politics',\n",
    "                    'brexit': 'British exit'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting some help from ekphrasis\n",
    "\n",
    "We will normalize several type of texts as mentioned below and annotate some part of the text which is useful for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "# for removing these converted tags after processing \n",
    "replacement_dict1 = {'<url>':'', \n",
    "             '<hashtag>':'',\n",
    "             '</hashtag>':'',\n",
    "             '<elongated>':'',\n",
    "             '<repeated>':'',\n",
    "             '<emphasis>':'',\n",
    "             '<user>':''}\n",
    "    \n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number', 'censored'],\n",
    "    \n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"elongated\", \"repeated\",\n",
    "        'emphasis'},\n",
    "    \n",
    "    fix_html=True,  # fix HTML tokens like '&lt; &gt; &amp; \\xa0\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=False,  # Unpack contractions (can't -> can not) (Will do explicitly)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # Social tokenizer, will take a string  as input and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions.\n",
    "    dicts=[emoticons, replacement_dict1] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the below function will consists of steps to be carried for preprocessing the xmls.\n",
    "\n",
    "Each xml contains several documents, preprocessing will be done document wise and will be stored in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special non ascii characters/emojis and some mappings \n",
    "def transform_doc(doc):\n",
    "    '''\n",
    "    preprocess each document for modelling\n",
    "    @arg1: doc - document text to be preprocessed, dtype: String\n",
    "    return: doc - preprocessed document, dtype: String\n",
    "    '''\n",
    "    \n",
    "    # replace special characters from above dictionary \n",
    "    for char, replacement in special_dict.items():\n",
    "        doc = doc.replace(char, replacement)\n",
    "    \n",
    "    # emoji replacement\n",
    "    for emo, equivalent in UNICODE_EMO.items():\n",
    "        doc = doc.replace(emo,equivalent).replace('_',' ')\n",
    "    \n",
    "    doc = doc.encode('ascii', \"ignore\").decode() # remove unnecessary encoded characters/words\n",
    "    \n",
    "    # removal of special urls\n",
    "    url_pattern2 = re.compile(r'https?/\\S+|www\\.\\S+')\n",
    "    doc = url_pattern2.sub(r'', doc)\n",
    "    \n",
    "    # expand contractions\n",
    "    doc = contractions.fix(doc)\n",
    "    \n",
    "    # preprocess all steps mentioned in above TextPreprocessor using ekphrasis\n",
    "    doc = \" \".join(text_processor.pre_process_doc(doc))\n",
    "    \n",
    "    # remove unnecessary special characters\n",
    "    doc = re.sub('[^A-Za-z]+', ' ', doc)\n",
    "    \n",
    "    # replacing words found by coverage experiment\n",
    "    for word, replacement in replacement_dict.items():\n",
    "        doc = doc.replace(word, replacement)\n",
    "    \n",
    "    # remove extra spaces between words trims and remove extra \\n\n",
    "    doc = \" \".join(doc.split())\n",
    "    \n",
    "    # convert to lower case\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess using above training and testing data using above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over each xml on training data for preprocessing\n",
    "for xml_file in document_dict.keys(): \n",
    "    transformed_docs = []\n",
    "    # transform each document for xml file using above function\n",
    "    for doc in document_dict[xml_file]:\n",
    "        transformed_docs.append(transform_doc(doc))\n",
    "\n",
    "    document_dict.update({xml_file: transformed_docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform similar processing for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over each xml on test data for preprocessing\n",
    "for xml_file in document_test_dict.keys(): \n",
    "    transformed_docs = []\n",
    "    # transform each document for xml file using above function\n",
    "    for doc in document_test_dict[xml_file]:\n",
    "        transformed_docs.append(transform_doc(doc))\n",
    "\n",
    "    document_test_dict.update({xml_file: transformed_docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will convert this dictionary of transformed text into a dataframe.\n",
    "\n",
    "The dataframe will be in a melted form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting gender series to dictinary with id as index\n",
    "label_lookup = pd.Series(train_labels_df.gender.values,index=train_labels_df.id).to_dict()\n",
    "\n",
    "train_data = list()\n",
    "\n",
    "# iterate over documents to build a list of tuples\n",
    "for id in train_files:\n",
    "    train_data.extend([tuple((id, doc, label_lookup[id])) for doc in document_dict[id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of tuples to dataframe\n",
    "train_df = pd.DataFrame(train_data, columns=['id', 'document','gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform similar steps on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting gender series to dictinary with id as index\n",
    "label_lookup = pd.Series(test_labels_df.gender.values,index=test_labels_df.id).to_dict()\n",
    "\n",
    "test_data = list()\n",
    "\n",
    "# iterate over documents to build a list of tuples\n",
    "for id in test_files:\n",
    "    test_data.extend([tuple((id, doc, label_lookup[id])) for doc in document_test_dict[id]])\n",
    "\n",
    "# convert list of tuples to dataframe\n",
    "test_df = pd.DataFrame(test_data, columns=['id', 'document','gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('training_data_dl.csv', index=False)\n",
    "test_df.to_csv('testing_data_dl.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the preprocessing steps carried till now were just developed so that features from text can be extracted using embeddings for each word or sentence. \n",
    "\n",
    "It is important to understand that preprocessing steps like stopwords removal or removing less/more frequent words should not be carried if we were to extract emebdding features as context of the word/sentence may be lost if we carry these steps.\n",
    "\n",
    "For syntax understanding, you need to either leave in the stop words or alter your stop list, such that you don't lose that information. For instance, cutting out all verbs of being (is, are, should be, ...) can mess up a NN that depends somewhat on sentence structure.\n",
    "\n",
    "We will now carry remaining steps for text preprocessing specific to Tf-IDf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pre-processing for TfIdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets grab stopwords from nltk, instead of importing, I have just copied it from nltk package and adjusted this list a bit to suit our need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = {'a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "             'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \n",
    "             \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during',\n",
    "             'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", \n",
    "             'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \n",
    "             \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn',\n",
    "             \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or',\n",
    "             'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should',\n",
    "             \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very',\n",
    "             'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will',\n",
    "             'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'face', 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>youch good things know sort stuff repairable</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>succumbed fomo bought gnr tickets remember ask...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>brown eye broom cool number rescue clear broke...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>shout auckland tennis fans get sleep morning w...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>someone balls come tears joy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  d7d392835f50664fc079f0f388e147a0   \n",
       "1  d7d392835f50664fc079f0f388e147a0   \n",
       "2  d7d392835f50664fc079f0f388e147a0   \n",
       "3  d7d392835f50664fc079f0f388e147a0   \n",
       "4  d7d392835f50664fc079f0f388e147a0   \n",
       "\n",
       "                                            document gender  \n",
       "0       youch good things know sort stuff repairable   male  \n",
       "1  succumbed fomo bought gnr tickets remember ask...   male  \n",
       "2  brown eye broom cool number rescue clear broke...   male  \n",
       "3  shout auckland tennis fans get sleep morning w...   male  \n",
       "4                       someone balls come tears joy   male  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "# remove stopwords from train data\n",
    "train_df[\"document\"] = train_df[\"document\"].apply(lambda text: remove_stopwords(text))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>odds stops whining goes gets proper job like r...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>would imagine moderately pleased draw</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>worth reading blog nick positive must admit tr...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>hand take last race beaten number lengths jump...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>certainly showing interest reading</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  d6b08022cdf758ead05e1c266649c393   \n",
       "1  d6b08022cdf758ead05e1c266649c393   \n",
       "2  d6b08022cdf758ead05e1c266649c393   \n",
       "3  d6b08022cdf758ead05e1c266649c393   \n",
       "4  d6b08022cdf758ead05e1c266649c393   \n",
       "\n",
       "                                            document gender  \n",
       "0  odds stops whining goes gets proper job like r...   male  \n",
       "1              would imagine moderately pleased draw   male  \n",
       "2  worth reading blog nick positive must admit tr...   male  \n",
       "3  hand take last race beaten number lengths jump...   male  \n",
       "4                 certainly showing interest reading   male  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords from test data\n",
    "test_df[\"document\"] = test_df[\"document\"].apply(lambda text: remove_stopwords(text))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will first find most frequent words present in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('number', 44605),\n",
       " ('amp', 14117),\n",
       " ('like', 12256),\n",
       " ('one', 12016),\n",
       " ('time', 11304),\n",
       " ('trump', 10876),\n",
       " ('new', 10528),\n",
       " ('would', 10338),\n",
       " ('get', 10244),\n",
       " ('good', 9984),\n",
       " ('joy', 9839),\n",
       " ('tears', 9802),\n",
       " ('smiling', 9547),\n",
       " ('day', 9524),\n",
       " ('heart', 9211)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = Counter()\n",
    "\n",
    "for text in train_df[\"document\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "\n",
    "cnt.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove these words from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>youch things know sort stuff repairable</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>succumbed fomo bought gnr tickets remember ask...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>brown eye broom cool rescue clear broken windo...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>shout auckland tennis fans sleep morning worth...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>someone balls come</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  d7d392835f50664fc079f0f388e147a0   \n",
       "1  d7d392835f50664fc079f0f388e147a0   \n",
       "2  d7d392835f50664fc079f0f388e147a0   \n",
       "3  d7d392835f50664fc079f0f388e147a0   \n",
       "4  d7d392835f50664fc079f0f388e147a0   \n",
       "\n",
       "                                            document gender  \n",
       "0            youch things know sort stuff repairable   male  \n",
       "1  succumbed fomo bought gnr tickets remember ask...   male  \n",
       "2  brown eye broom cool rescue clear broken windo...   male  \n",
       "3  shout auckland tennis fans sleep morning worth...   male  \n",
       "4                                 someone balls come   male  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(15)])\n",
    "\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "# remove freqwords from training data\n",
    "train_df[\"document\"] = train_df[\"document\"].apply(lambda text: remove_freqwords(text))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets extract rare words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('beardyman', 1),\n",
       " ('bulmers', 1),\n",
       " ('drumkits', 1),\n",
       " ('forbidding', 1),\n",
       " ('haptic', 1),\n",
       " ('hingmy', 1),\n",
       " ('maalouf', 1),\n",
       " ('mayzing', 1),\n",
       " ('mcx', 1),\n",
       " ('moldavia', 1),\n",
       " ('pusilanimous', 1),\n",
       " ('satpal', 1),\n",
       " ('semen', 1),\n",
       " ('shitport', 1),\n",
       " ('shitstorms', 1),\n",
       " ('sozz', 1),\n",
       " ('tiddleywinks', 1),\n",
       " ('traid', 1),\n",
       " ('wighty', 1),\n",
       " ('zucks', 1)}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([(w,wc) for (w, wc) in cnt.most_common()[:-20-1:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove rare words now, from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>youch things know sort stuff repairable</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>succumbed fomo bought gnr tickets remember ask...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>brown eye broom cool rescue clear broken windo...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>shout auckland tennis fans sleep morning worth...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>someone balls come</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  d7d392835f50664fc079f0f388e147a0   \n",
       "1  d7d392835f50664fc079f0f388e147a0   \n",
       "2  d7d392835f50664fc079f0f388e147a0   \n",
       "3  d7d392835f50664fc079f0f388e147a0   \n",
       "4  d7d392835f50664fc079f0f388e147a0   \n",
       "\n",
       "                                            document gender  \n",
       "0            youch things know sort stuff repairable   male  \n",
       "1  succumbed fomo bought gnr tickets remember ask...   male  \n",
       "2  brown eye broom cool rescue clear broken windo...   male  \n",
       "3  shout auckland tennis fans sleep morning worth...   male  \n",
       "4                                 someone balls come   male  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rare_words = 20\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]]) # set of rare_words\n",
    "\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "# remove rare words from training data\n",
    "train_df[\"document\"] = train_df[\"document\"].apply(lambda text: remove_rarewords(text))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform this step for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('number', 7503),\n",
       " ('amp', 2216),\n",
       " ('trump', 1977),\n",
       " ('one', 1951),\n",
       " ('like', 1861),\n",
       " ('get', 1820),\n",
       " ('time', 1756),\n",
       " ('new', 1752),\n",
       " ('would', 1744),\n",
       " ('joy', 1740),\n",
       " ('tears', 1738),\n",
       " ('smiling', 1526),\n",
       " ('good', 1515),\n",
       " ('day', 1460),\n",
       " ('happy', 1414)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = Counter()\n",
    "\n",
    "for text in test_df[\"document\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "\n",
    "cnt.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>odds stops whining goes gets proper job rest us</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>imagine moderately pleased draw</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>worth reading blog nick positive must admit tr...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>hand take last race beaten lengths jumps recen...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>certainly showing interest reading</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  d6b08022cdf758ead05e1c266649c393   \n",
       "1  d6b08022cdf758ead05e1c266649c393   \n",
       "2  d6b08022cdf758ead05e1c266649c393   \n",
       "3  d6b08022cdf758ead05e1c266649c393   \n",
       "4  d6b08022cdf758ead05e1c266649c393   \n",
       "\n",
       "                                            document gender  \n",
       "0    odds stops whining goes gets proper job rest us   male  \n",
       "1                    imagine moderately pleased draw   male  \n",
       "2  worth reading blog nick positive must admit tr...   male  \n",
       "3  hand take last race beaten lengths jumps recen...   male  \n",
       "4                 certainly showing interest reading   male  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(15)])\n",
    "\n",
    "# remove most frequent words in testing data\n",
    "test_df[\"document\"] = test_df[\"document\"].apply(lambda text: remove_freqwords(text))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aby',\n",
       " 'adedapo',\n",
       " 'bellow',\n",
       " 'boycot',\n",
       " 'cojones',\n",
       " 'endment',\n",
       " 'endmentnow',\n",
       " 'fooling',\n",
       " 'gbollmann',\n",
       " 'imbecile',\n",
       " 'impeaching',\n",
       " 'impeachtrumpandpence',\n",
       " 'julios',\n",
       " 'kellyann',\n",
       " 'marxova',\n",
       " 'montrel',\n",
       " 'petkanas',\n",
       " 'santuary',\n",
       " 'trumperers',\n",
       " 'wont'}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([w for (w, wc) in cnt.most_common()[:-20-1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>odds stops whining goes gets proper job rest us</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>imagine moderately pleased draw</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>worth reading blog nick positive must admit tr...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>hand take last race beaten lengths jumps recen...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>certainly showing interest reading</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  d6b08022cdf758ead05e1c266649c393   \n",
       "1  d6b08022cdf758ead05e1c266649c393   \n",
       "2  d6b08022cdf758ead05e1c266649c393   \n",
       "3  d6b08022cdf758ead05e1c266649c393   \n",
       "4  d6b08022cdf758ead05e1c266649c393   \n",
       "\n",
       "                                            document gender  \n",
       "0    odds stops whining goes gets proper job rest us   male  \n",
       "1                    imagine moderately pleased draw   male  \n",
       "2  worth reading blog nick positive must admit tr...   male  \n",
       "3  hand take last race beaten lengths jumps recen...   male  \n",
       "4                 certainly showing interest reading   male  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]]) # set of rare_words\n",
    "\n",
    "# remove rare words from testing data\n",
    "test_df[\"document\"] = test_df[\"document\"].apply(lambda text: remove_rarewords(text))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use lemmatization here.\n",
    "\n",
    "WordNetLemmatizer is used for preprocessing, and part of speech tagging is used from nltk to get POS for each word so that converting word to root form is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>youch thing know sort stuff repairable</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>succumb fomo buy gnr ticket remember ask paren...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>brown eye broom cool rescue clear break window...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>shout auckland tennis fan sleep morning worth ...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>someone ball come</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  d7d392835f50664fc079f0f388e147a0   \n",
       "1  d7d392835f50664fc079f0f388e147a0   \n",
       "2  d7d392835f50664fc079f0f388e147a0   \n",
       "3  d7d392835f50664fc079f0f388e147a0   \n",
       "4  d7d392835f50664fc079f0f388e147a0   \n",
       "\n",
       "                                            document gender  \n",
       "0             youch thing know sort stuff repairable   male  \n",
       "1  succumb fomo buy gnr ticket remember ask paren...   male  \n",
       "2  brown eye broom cool rescue clear break window...   male  \n",
       "3  shout auckland tennis fan sleep morning worth ...   male  \n",
       "4                                  someone ball come   male  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# laod wordnet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS mapping\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    '''custom function to get part of speech tagged text\n",
    "    and convert it into root form'''\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "# all text from training data is converted to lemmatized form \n",
    "train_df[\"document\"] = train_df[\"document\"].apply(lambda text: lemmatize_words(text))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>odds stop whine go get proper job rest u</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>imagine moderately pleased draw</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>worth read blog nick positive must admit trust...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>hand take last race beat lengths jump recency ...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>certainly show interest reading</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  d6b08022cdf758ead05e1c266649c393   \n",
       "1  d6b08022cdf758ead05e1c266649c393   \n",
       "2  d6b08022cdf758ead05e1c266649c393   \n",
       "3  d6b08022cdf758ead05e1c266649c393   \n",
       "4  d6b08022cdf758ead05e1c266649c393   \n",
       "\n",
       "                                            document gender  \n",
       "0           odds stop whine go get proper job rest u   male  \n",
       "1                    imagine moderately pleased draw   male  \n",
       "2  worth read blog nick positive must admit trust...   male  \n",
       "3  hand take last race beat lengths jump recency ...   male  \n",
       "4                    certainly show interest reading   male  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all text from test data is converted to lemmatized form \n",
    "test_df[\"document\"] = test_df[\"document\"].apply(lambda text: lemmatize_words(text))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These dataframes are saved as csv which can be used for feature extraction using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('training_data_ml.csv', index=False)\n",
    "test_df.to_csv('testing_data_ml.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) https://towardsdatascience.com/why-you-should-avoid-removing-stopwords-aa7a353d2a52\n",
    "\n",
    "2) UNICODE_EMO dictionary, by Neel Shah https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "\n",
    "3) Contractions: a pypi library from http://pypi.org/project/contractions/\n",
    "\n",
    "4) Ekphrasis: A text-preprocessing tool from https://github.com/cbaziotis/ekphrasis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
