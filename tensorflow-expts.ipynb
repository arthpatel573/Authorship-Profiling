{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBysnCOIUzpT"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, GRU,SimpleRNN\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import Embedding, BatchNormalization\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, Conv2D, MaxPool1D, Concatenate,MaxPool2D, Flatten, Bidirectional, SpatialDropout1D, Reshape, Input, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing import sequence, text\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import datetime\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hiWf3WAxUzpa",
    "outputId": "bd28c5a4-9bdc-43dd-ee71-6d3dd9546b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Default distribution strategy for one GPU or CPUs \n",
    "strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5WhasUicWnZ"
   },
   "outputs": [],
   "source": [
    "# !unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6a-pfhSJ8VG6"
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of training set we developed from preprocess notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "vIAZiDl4Uzpc",
    "outputId": "265aba30-61ec-48fd-d18b-b57ad600e5ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>youch good things to know is that sort of stuf...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>succumbed to fomo and bought gnr tickets . rem...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>brown eye broom a cool number then to the resc...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>shout out to auckland tennis fans who get to s...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>someone had some balls to come up with that</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  ... gender\n",
       "0  d7d392835f50664fc079f0f388e147a0  ...   male\n",
       "1  d7d392835f50664fc079f0f388e147a0  ...   male\n",
       "2  d7d392835f50664fc079f0f388e147a0  ...   male\n",
       "3  d7d392835f50664fc079f0f388e147a0  ...   male\n",
       "4  d7d392835f50664fc079f0f388e147a0  ...   male\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('training_data_dl.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4ooqcKbUzpi"
   },
   "outputs": [],
   "source": [
    "train_df['gender'] = train_df['gender'].apply(lambda x: 1 if x=='male' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DiEiGfDw7T6"
   },
   "outputs": [],
   "source": [
    "xml_df = train_df[['id','gender']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cQpp93_Sw2Vx"
   },
   "outputs": [],
   "source": [
    "Xml_train, Xml_test, y_train, y_test = train_test_split(xml_df['id'].values, xml_df['gender'].values,\n",
    "                                                        random_state=123,\n",
    "                                                        shuffle=True, \n",
    "                                                        test_size=0.2,\n",
    "                                                        stratify=xml_df['gender'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFGMCNbGUzpg"
   },
   "outputs": [],
   "source": [
    "train_df.dropna(subset=['document'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "G2X5Po68Uzpk",
    "outputId": "e962ab0d-f4b7-4fa3-a12c-5b5ead7e0e67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the maximum size of document to get an estimate of padding at a later stage\n",
    "train_df['document'].apply(lambda x:len(str(x).split())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCExJc0fxTFZ"
   },
   "outputs": [],
   "source": [
    "xtrain = train_df.loc[train_df['id'].isin(Xml_train),'document'].values\n",
    "ytrain = train_df.loc[train_df['id'].isin(Xml_train),'gender'].values\n",
    "\n",
    "xvalid = train_df.loc[~train_df['id'].isin(Xml_train),'document'].values\n",
    "yvalid = train_df.loc[~train_df['id'].isin(Xml_train),'gender'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CFTwKhw_Uzpo",
    "outputId": "7c31ef35-2653-4b46-ac9e-72a2f5205b30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'donald the menace thanks comey'"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ya2ZTpWLUzpq"
   },
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 45\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "#zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3Y31qFMUzps"
   },
   "source": [
    " We represent every word as one hot vectors of dimensions : Numbers of words in Vocab +1.\n",
    "What keras Tokenizer does is , it takes all the unique words in the corpus,forms a dictionary with words as keys and their number of occurences as values,it then sorts the dictionary in descending order of counts. It then assigns the first value 1 , second value 2 and so on. So let's suppose word 'the' occured the most in the corpus then it will assigned index 1 and vector representing 'the' would be a one-hot vector with value 1 at position 1 and rest zereos.\n",
    "Try printing first 2 elements of xtrain_seq you will see every word is represented as a digit now\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0Py4rmxkUzpt",
    "outputId": "3be5ede3-23a1-4b7e-ec5f-8a14b5e5a1d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XSK66UE0Uzpv",
    "outputId": "208c861b-348e-49b3-e531-8518868ee1a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51, 2, 520, 135, 1413, 20, 58, 481]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pi3mduk3FDSS"
   },
   "source": [
    "Following piece of code is for reference. It was used to perform experiments using GloVe Embeddings. Unfortunately due to gender bias in the learned embeddings, models performed did not work well they were biased towards female class [Read more](http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n",
    "\n",
    "Some other literature if you are interested:\n",
    "\n",
    "1) [Stanford paper](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6835575.pdf)\n",
    "\n",
    "2) [ACL paper](https://www.aclweb.org/anthology/P19-1160.pdf)\n",
    "\n",
    "3) [University of Toronto](https://arxiv.org/pdf/1810.03611.pdf)\n",
    "\n",
    "```python\n",
    "# GloVe vectors loading into dictionary:\n",
    "\n",
    "# downloaded from  http://www-nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "embeddings_index = {}\n",
    "f = open(r'glove.840B.300d.txt','r',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray([float(vala for val in values[1:]])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "```\n",
    "\n",
    "Following confusion matrix was obtained on models developed using GLoVe:\n",
    "\n",
    "1) 1DCNN:\n",
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
    "array([[241,  11],\n",
    "       [180, 68]], dtype=int32)>\n",
    "```\n",
    "2) BiLSTM:\n",
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
    "array([[244,  8],\n",
    "       [175, 73]], dtype=int32)>\n",
    "```\n",
    "2) GRU:\n",
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
    "array([[235,  17],\n",
    "       [165, 83]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_df[\"document\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100%|██████████████████████████████████████████████████████████████████████| 309990/309990 [00:00<00:00, 398982.41it/s]<br>\n",
    "100%|██████████████████████████████████████████████████████████████████████| 309990/309990 [00:00<00:00, 397466.71it/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage experiment\n",
    "\n",
    "Here we will find how much percentage of words are covered by GloVe dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary (oov) words that we can use to improve our preprocessing\n",
    "\n",
    "import operator \n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)\n",
    "oov[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found embeddings for 76.24% of vocab<br>\n",
    "Found embeddings for  99.13% of all text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three models develped below uses learned embeddings. Glove can be used in the below models by passing `embedding_matrix` and `trainable=False` parameter in the first layer of all below models but as mentioned earlier, it won't give good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1DsKpGU79xN"
   },
   "source": [
    "# BiLSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yi6gBhkFeEL0"
   },
   "source": [
    "Bidirectional LSTM based architecture was chosen because this type of layers, can look through in both the direction: forward and backward. Hence, it is very useful for text classfication where importance of word is determined by the words around it in both direction.\n",
    "\n",
    "The hyperparameter for these type of architectures were tuned manually, as I didn't have high computation power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2o6ChqLvUzp2"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "Yz7ZsuSBUzqD",
    "outputId": "3c19b0c5-12c2-4fe2-b83f-f0d6f0e28c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 45, 300)           27567900  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 45, 256)           439296    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               164352    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 28,175,709\n",
      "Trainable params: 28,175,709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 1.46 s, sys: 20 ms, total: 1.48 s\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    \n",
    "    # A LSTM with custom embeddings\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     input_length=max_len))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.2, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.2)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "          optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "          metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAtUBdOpe7Fd"
   },
   "source": [
    "Experiments were carried with different values of batch size and epoch till it converge or till the time when validation loss stops improving/fluctuates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(xtrain_pad,\n",
    "                    ytrain,\n",
    "                    epochs=5,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(xvalid_pad, yvalid),\n",
    "                    verbose=1)\n",
    "                    # callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1Dz9g-MfTCG"
   },
   "source": [
    "As this didn't give better performance, it was neglected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QrPfrsyF8Dfo"
   },
   "source": [
    "# 1DCNN model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e99bjN6Pfc8N"
   },
   "source": [
    "Other approach was taken from [Paragraph classification](http://cs229.stanford.edu/proj2016/report/NhoNg-ParagraphTopicClassification-report.pdf) where multilayered Convolutional layers with maxpooling operation are used.\n",
    "\n",
    "Convolution layers applies filterss through the rows of representation/embeddings for sentences, and learns important information. Size of this filter indicates how far to look in a window for aggregration using Maxpooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMwoxvU28I-z"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IeLwESDUzqN"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "filter_sizes = [1,3,5]\n",
    "num_filters = 50\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(max_len, ))\n",
    "    x = Embedding(len(word_index) + 1,\n",
    "                  embed_size)(inp)\n",
    "   \n",
    "    conv_0 = Conv1D(num_filters, kernel_size=(filter_sizes[0]),\n",
    "                                 kernel_initializer='he_normal', activation='tanh')(x)\n",
    "    conv_1 = Conv1D(num_filters, kernel_size=(filter_sizes[1]),\n",
    "                                 kernel_initializer='he_normal', activation='tanh')(x)\n",
    "    conv_2 = Conv1D(num_filters, kernel_size=(filter_sizes[2]), \n",
    "                                 kernel_initializer='he_normal', activation='tanh')(x)\n",
    "    \n",
    "    maxpool_0 = MaxPool1D(pool_size=(max_len - filter_sizes[0] + 1))(conv_0)\n",
    "    maxpool_1 = MaxPool1D(pool_size=(max_len - filter_sizes[1] + 1))(conv_1)\n",
    "    maxpool_2 = MaxPool1D(pool_size=(max_len - filter_sizes[2] + 1))(conv_2)\n",
    "        \n",
    "    z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.2)(z) \n",
    "    z = Dense(32, activation = 'relu')(z)\n",
    "    z = Dropout(0.4)(z)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655
    },
    "colab_type": "code",
    "id": "7axOCzAsUzqQ",
    "outputId": "b347ce7a-d610-4d0b-84a0-59c7de4336f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 45, 300)      27567900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 45, 50)       15050       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 43, 50)       45050       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 41, 50)       75050       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1, 50)        0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 50)        0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 50)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3, 50)        0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 150)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 150)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           4832        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            33          dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 27,707,915\n",
      "Trainable params: 27,707,915\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "hgyduE6MAOrk",
    "outputId": "e3a46a65-f01f-4d17-fee9-f10a3d6ff24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1938/1938 [==============================] - 593s 306ms/step - loss: 0.6745 - accuracy: 0.5735 - val_loss: 0.6623 - val_accuracy: 0.6027\n",
      "Epoch 2/3\n",
      "1938/1938 [==============================] - 588s 304ms/step - loss: 0.6426 - accuracy: 0.6685 - val_loss: 0.6611 - val_accuracy: 0.6201\n",
      "Epoch 3/3\n",
      "1938/1938 [==============================] - 590s 304ms/step - loss: 0.6224 - accuracy: 0.7166 - val_loss: 0.6625 - val_accuracy: 0.6292\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xtrain_pad,\n",
    "                    ytrain,\n",
    "                    epochs=3, # optimize this value \n",
    "                    batch_size=128,\n",
    "                    validation_data=(xvalid_pad, yvalid),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 3 epochs are logs are provided here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1OAM6C9gSevH"
   },
   "source": [
    "# 2D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hziNiQVFhGFP"
   },
   "source": [
    "Finally, inspired from [Gender Classification approach](https://www.mdpi.com/2076-3417/9/6/1249) architecture, 2 dimentional filters were used to get global meaning from a sentence learning from emebedding of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQME_g2hutd2"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKRN2jTuSfUM"
   },
   "outputs": [],
   "source": [
    "filter_sizes = [1,2,3,5]\n",
    "num_filters = 42\n",
    "embed_size = 300\n",
    "\n",
    "def get_model():    \n",
    "    inp = Input(shape=(max_len, ))\n",
    "    x = Embedding(len(word_index) + 1, embed_size)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Reshape((max_len, embed_size, 1))(x)\n",
    "    \n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    \n",
    "    maxpool_0 = MaxPool2D(pool_size=(max_len - filter_sizes[0] + 1, 1))(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(max_len - filter_sizes[1] + 1, 1))(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(max_len - filter_sizes[2] + 1, 1))(conv_2)\n",
    "    maxpool_3 = MaxPool2D(pool_size=(max_len - filter_sizes[3] + 1, 1))(conv_3)\n",
    "        \n",
    "    z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "        \n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "colab_type": "code",
    "id": "tPHmGmjlV-7o",
    "outputId": "2624e45e-123c-42fe-ae2b-c403445c3677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 45, 300)      27567900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 45, 300)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 45, 300, 1)   0           spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 45, 1, 42)    12642       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 44, 1, 42)    25242       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 43, 1, 42)    37842       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 41, 1, 42)    63042       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 42)     0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 42)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 42)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 42)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 4, 1, 42)     0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 168)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 168)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            169         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 27,706,837\n",
      "Trainable params: 138,937\n",
      "Non-trainable params: 27,567,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "exjTL1pfVXvn",
    "outputId": "b8cd5525-b28b-4dbd-c59b-cb83a1a21309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "969/969 [==============================] - 318s 328ms/step - loss: 0.6706 - accuracy: 0.5825 - val_loss: 0.6750 - val_accuracy: 0.5754\n",
      "Epoch 2/10\n",
      "969/969 [==============================] - 317s 327ms/step - loss: 0.6382 - accuracy: 0.6779 - val_loss: 0.6779 - val_accuracy: 0.5914\n",
      "Epoch 3/10\n",
      "969/969 [==============================] - 315s 325ms/step - loss: 0.6155 - accuracy: 0.7344 - val_loss: 0.6820 - val_accuracy: 0.5925\n",
      "Epoch 4/10\n",
      "969/969 [==============================] - 317s 327ms/step - loss: 0.5993 - accuracy: 0.7727 - val_loss: 0.6841 - val_accuracy: 0.5922\n",
      "Epoch 5/10\n",
      "969/969 [==============================] - 319s 329ms/step - loss: 0.5892 - accuracy: 0.7963 - val_loss: 0.6842 - val_accuracy: 0.5959\n",
      "Epoch 6/10\n",
      "969/969 [==============================] - 319s 329ms/step - loss: 0.5821 - accuracy: 0.8125 - val_loss: 0.6848 - val_accuracy: 0.5960\n",
      "Epoch 7/10\n",
      "969/969 [==============================] - 318s 328ms/step - loss: 0.5774 - accuracy: 0.8235 - val_loss: 0.6846 - val_accuracy: 0.5914\n",
      "Epoch 8/10\n",
      "969/969 [==============================] - 317s 327ms/step - loss: 0.5736 - accuracy: 0.8324 - val_loss: 0.6861 - val_accuracy: 0.5938\n",
      "Epoch 9/10\n",
      "969/969 [==============================] - 317s 327ms/step - loss: 0.5709 - accuracy: 0.8389 - val_loss: 0.6864 - val_accuracy: 0.5936\n",
      "Epoch 10/10\n",
      "969/969 [==============================] - 317s 327ms/step - loss: 0.5683 - accuracy: 0.8446 - val_loss: 0.6895 - val_accuracy: 0.5933\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xtrain_pad,\n",
    "                    ytrain,\n",
    "                    epochs=10, # should be optimized \n",
    "                    batch_size=256, # should be optimized\n",
    "                    validation_data=(xvalid_pad, yvalid),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6DDiGd4RLTM"
   },
   "source": [
    "### Normalized prob testing\n",
    "\n",
    "Probability for each xml was determined by averaging prediction probability of all documents to get normalized value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv('testing_data_dl.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove duplicates and convert target row into numbers, with ```male:0 and female:1```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FP8FwDeWexml"
   },
   "outputs": [],
   "source": [
    "result_df['gender'] = result_df['gender'].apply(lambda x: 1 if x=='male' else 0) # number conversion of target\n",
    "result_df.dropna(subset=['document'], inplace=True) # drop na\n",
    "result_ids = result_df['id'].unique() # get ids for all test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = []\n",
    "target = []\n",
    "success = 0\n",
    "for id in result_ids:\n",
    "    xtest = result_df.loc[result_df['id']==id,'document'].values\n",
    "    label = result_df.loc[result_df['id']==id,'gender'].iloc[0]\n",
    "    xtest_seq = token.texts_to_sequences(xtest)\n",
    "    xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "    test_prob = np.mean(model.predict(xtest_pad))\n",
    "    if (test_prob>0.5 and label==1) or (test_prob<0.5 and label==0):\n",
    "        success+=1\n",
    "    if test_prob>0.5:\n",
    "        pred_result.append(1)\n",
    "    else:\n",
    "        pred_result.append(0)\n",
    "    target.append(label)\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 1D CNN architecture\n",
    "# accuracy was: 68%\n",
    "tf.math.confusion_matrix(labels=target, predictions=pred_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2D CNN architecture\n",
    "# accuracy was: 68.6%\n",
    "tf.math.confusion_matrix(labels=target, predictions=pred_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other experiments were performed based on Universal Sentence Encoder(USE), I have used this encoder in many text classification taks and they seem to be very powerful, they are based on a famous attention based mechanism [Attention is All you need](https://arxiv.org/abs/1706.03762) \n",
    "\n",
    "However, it didn't performed very well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tf.keras.Input((), dtype = tf.string, name = 'input_text')\n",
    "# load embedding module\n",
    "embed_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "# convert it into a tensorflow layer\n",
    "embedding_layer = hub.KerasLayer(embed_use, input_shape = [],\n",
    "                           dtype = tf.string,\n",
    "                           trainable = False)(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a feeed forward network is build on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(0.005),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 247987 samples, validate on 61993 samples<br>\n",
    "Epoch 1/5<br>\n",
    "247987/247987 [==============================] - 35s 142us/sample - loss: 0.6686 - accuracy: 0.5854 - val_loss: 0.6737 - <br>val_accuracy: 0.6253<br>\n",
    "Epoch 2/5<br>\n",
    "247987/247987 [==============================] - 32s 128us/sample - loss: 0.6625 - accuracy: 0.6060 - val_loss: 0.6746 - <br>val_accuracy: 0.6314<br>\n",
    "Epoch 3/5<br>\n",
    "247987/247987 [==============================] - 32s 128us/sample - loss: 0.6590 - accuracy: 0.6179 - val_loss: 0.6734 - <br>val_accuracy: 0.6290<br>\n",
    "Epoch 4/5<br>\n",
    "247987/247987 [==============================] - 32s 128us/sample - loss: 0.6567 - accuracy: 0.6258 - val_loss: 0.6746 - <br>val_accuracy: 0.6231<br>\n",
    "Epoch 5/5<br>\n",
    "247987/247987 [==============================] - 32s 129us/sample - loss: 0.6536 - accuracy: 0.6345 - val_loss: 0.6752 - <br>val_accuracy: 0.6276<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can se val_accuracy decrease and fluctuates even after tuning hyperparamter for layer size and dropout values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NML-AnTDh7fe"
   },
   "source": [
    "After some research, found that more extensive research and sophisticated architecture needs to be developed to solve particulary this type of classification problem by avoiding gender bias issue in the learned embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final version of the model can be found in `./scripts/train.py` which can be deployed on any cloud platform.\n",
    "\n",
    "Here, we will deploy it on **AWS SageMaker** with custom container. Please read `./deploy.ipynb` file for instructions on deploying this TF model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQhRXSEAUzrM"
   },
   "source": [
    "Following experiments will be considered for future work.\n",
    "\n",
    "# ToDo:\n",
    "\n",
    "1. Spell checker and analyze mispelled ones in Text preprocessing\n",
    "2. Hyperparameter tuning with LSTM different cells\n",
    "3. Try an ensemble kind of structure for CNN and LSTM\n",
    "4. Roberta implementation \n",
    "5. Attention with BiLSTM \n",
    "\n",
    "### Code formatting:\n",
    "1. tensorflow data API for train, valid batch sets like https://www.tensorflow.org/hub/tutorials/bangla_article_classifier\n",
    "2. Custom call with class for model if possible https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub_on_kaggle"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow-expt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}